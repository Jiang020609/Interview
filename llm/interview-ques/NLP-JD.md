# 如何实现“每句话都有引用”？

要让模型在生成的每一句话后面精准打上引用标签（如 [1]），通常有三种主流方案：

Prompt Engineering（最简单）： 在系统提示词中强制要求。示例 Prompt: “请根据提供的上下文回答问题。你的每一句话都必须以 [引用ID] 结尾，引用ID必须对应文中提供的文档编号。”

SFT（精调法）： 使用类似 LongCite 的方案。通过构造包含“句子-引用”对的数据集进行微调，让模型原生支持在句子结束时输出引用符号。

Post-processing（后处理）： 先让模型生成全文，然后利用一个小模型（或原模型）对每一句话进行 归因分析（Attribution），检索哪段上下文支持了这句话，再补上标签。

LongCite补充：

LongCite 的核心组件：CoC (Contextual Citation)，它要求模型生成的每一句话都必须自带一个精确到句子级别的索引。

LongCite 如何炼成的？

LongCite 的强大不在于它的模型架构（它可以用 GLM 或 Llama 作为基座），而在于它的一套自动化数据生产流水线：

第一步：自动化数据合成 (LongBench-Cite)

由于人类很难手动标注几万字文档中的每一句引用，团队开发了一个 Agent 流程：

分块抽取： 将长文档切成小块。

指令生成： 基于这些小块生成复杂问题。

多阶段归因： 让一个极其强大的模型（如 GPT-4o）反复核对，确定答案中的每一句话分别对应原文的哪一个 chunk_id。

得到数据集： 最终获得包含 10k+ 条高质量、超长上下文、带精确引用的监督微调（SFT）数据。

第二步：SFT 微调

使用这批数据对模型进行微调。微调后的模型（例如 LongCite-9B）学会了一种新技能：在生成答案时，它会主动在每句话末尾插入类似 (片段 12) 的标识。

第三步：部署与后处理

当模型输出 (片段 12) 时，系统会自动将这个 ID 映射回原始文档的高亮区域。

LongCite 的性能表现

高精度： 相比于通过 Prompt 强制要求引用，LongCite 这种经过原生微调的模型，其引用的**准确率（Faithfulness）**显著更高。

低开销： 它不需要在推理时进行多次搜索，而是一次性输出答案和引用，节省了 Token 和时间。

# 给你一个很大的模型，比如deepseek，很长的上下文，比如100k token，怎么来做一个deep search框架

构建 Deep Search 框架的核心思路是：将模型从“资料阅读器”转变为“自主调查员”。

传统的 RAG 像是在图书馆查字典（查到就回）；而 Deep Search 像是在写侦探报告（反复取证、推理、补漏）。

A. 查询拆解 (Query Decomposition)

模型接收到一个复杂问题（例如：“分析 2026 年全球固态电池产业链的最新技术突破及量产瓶颈”）

第一步是拒绝直接生成答案：子问题生成： 将大问题拆解为 3-5 个逻辑关联的子问题（技术路径、原材料供应、主要厂商进度、成本估算）。

确定搜索优先级： 哪些子问题是基础（如技术路径），必须先解决。

B. 长上下文注入 (Massive Context Injection)

DeepSeek-V3 的 MLA (Multi-head Latent Attention) 架构大幅降低了长上下文的推理成本。

原始证据喂入： 不同于传统 RAG 只取 300 字的切片（Chunk），Deep Search 利用 100k 的容量，将搜索到的 Top 10 网页经过 Markdown 清洗后全文塞入。

全局视野： 这保证了模型能看到文档内部的逻辑联系（例如：第一页提到的实验前提，对第五页的结论有什么影响），避免了断章取义。

推理循环：侦探式的“思考-行动” (Thought Loop)这是框架的大脑，利用 DeepSeek-R1 的 <thinking> 标签特性来实现。

思考阶段 (Thinking Phase)模型在内部进行自我审视和逻辑推演：“我已经根据搜索到的资料 A 知道了固态电池的能量密度，但资料 B 提到的充电循环次数和资料 C 矛盾。我需要寻找第三方独立测试机构的报告来验证，或者搜索该厂商最新的回复。”

行动阶段 (Action/Tool Use)根据思考结果，模型决定下一步：Search(new_query): 生成更精确的关键词进行第二轮、第三轮搜索。

Visit(url): 如果发现某篇深度文章里提到了一个 PDF 链接，模型会决定“深入”去抓取那个特定链接。

Synthesize: 如果判断信息已足够覆盖所有子问题，则停止循环。

State (状态) 存储目前已搜索到的所有 Markdown 原文、已确认的事实清单、尚待核实的疑点。

Node (节点)	负责执行特定逻辑：Query Generator -> Web Scraper -> Information Evaluator -> Writer

Edge (边/逻辑流) 关键逻辑判断：如果评估员认为“置信度 < 0.8”，则连线跳回搜索节点；否则连向生成节点。

解决“幻觉”的新方式： 幻觉往往源于信息缺失。Deep Search 强迫模型在“承认不知道”和“去搜索”之间选后者。

冲突处理 (Conflict Resolution)： 当网页 A 说 1 月量产，网页 B 说 6 月量产，模型会在 100k 上下文中进行交叉比对（Cross-Verification），查看哪个信源更权威、发布日期更新。

零推理延迟的“引用”： 因为所有参考资料都在 100k 上下文中，模型生成时可以实时标注 [1], [2]，由于不需要再次检索，这种引用的准确性极高。

# RAG基础知识

RAG 的标准动作（三个步骤）：

想象你要问 AI：“我们公司去年的团建去哪了？”

R (Retrieval) 检索：系统先去你的公司文档库里搜关键词“团建”、“去年”。它找到了一个 PDF，里面写着“2024年团建地点：三亚”。

A (Augmentation) 增强：系统把这段话塞进给 AI 的指令（Prompt）里，变成：“已知信息：2024年团建在三亚。请问：我们公司去年团建去哪了？”

G (Generation) 生成：AI 看了看这段话，很有底气地回答：“根据公司记录，去年团建去了三亚。”

RAG 的核心组件要搭一个 RAG，通常需要这几样东西：

数据源： 你的 PDF、Word、网页等。

切片 (Chunking)： 文档太长塞不下，得切成一段段。

向量数据库 (Vector DB)： 把文字变成“坐标（Embedding）”，方便电脑通过语义直接“定位”相关内容，而不是死板的关键词匹配。

大模型 (LLM)： 负责阅读搜到的片段并组织语言。

从 RAG 到 Deep Search

普通 RAG： “搜一次，答一次。”（像查字典）

Deep Search： “搜一次 -> 读一下 -> 发现没懂 -> 再搜一次 -> 逻辑推理 -> 给答案。”

RAG 是让 AI “有据可依”，而 Deep Search 是让 AI “学会利用证据”。、

# LoRA原理

LoRA (Low-Rank Adaptation)，即低秩自适应，是目前大模型微调领域最主流、最轻量化的技术。

它的核心思想可以用一句话概括：与其搬动整座大山，不如只给大山贴上一层轻便的“修正贴纸”。

核心数学原理：低秩分解在深度学习中，模型更新的本质是权重矩阵 $W$ 的变化。

传统的全参数微调是直接修改 $W$（参数量巨大）

而 LoRA 假设：模型在执行特定任务时，权重的变化（$\Delta W$）实际上不需要那么高的维度，它是“低秩”的。

因此，LoRA 不直接更新 $W$，而是将 $\Delta W$ 分解为两个极小的矩阵 $A$ 和 $B$ 的乘积：$$W_{updated} = W_{pre-trained} + \Delta W = W_{pre-trained} + (B \times A)$$
$W$ (冻结)：预训练模型的原始权重矩阵，大小为 $d \times k$。在训练过程中完全锁定，不更新。

$A$ (可训练)： 降维矩阵，大小为 $r \times k$。通常用高斯分布初始化。

$B$ (可训练)： 升维矩阵，大小为 $d \times r$。通常初始化为全 0，确保训练开始时 $\Delta W = 0$。

$r$ (秩/Rank)： 一个非常小的常数（如 8, 16, 64）。

LoRA 的工作流程冻结主干： 将原模型（如 DeepSeek, Llama）的所有参数设为不可训练。

旁路训练： 在原有的矩阵旁边“挂”上 $A$ 和 $B$ 两个小矩阵。训练时只计算这两个矩阵的梯度。

推理合并： 训练完成后，将 $B \times A$ 的结果加回原始权重 $W$ 中。

公式： $W_{new} = W + B \times A$

极省显存	因为只训练极少数参数（通常不到总量的 1%），普通的消费级显卡（如 RTX 4090）就能跑起百亿参数的大模型

无推理延迟推理时可以将 $B \times A$ 合并回主权重，模型结构不变，不会像其他插件式微调那样增加预测时间。

存储极小	训练出的“权重贴纸”（Adapter）通常只有几十 MB，而原模型有几十 GB。

灵活切换针对不同任务训练不同的 LoRA，底座模型共用，只需在推理时动态加载不同的 $A$ 和 $B$ 即可。

秩 $r$ (Rank) 的选择：

$r$ 代表了低秩矩阵的维度。常见取值： 4, 8, 16, 32, 64, 128, 256。

选择逻辑：任务复杂度： 如果是简单的风格转换或特定格式对齐（如：让模型学会写 JSON），$r=8$ 或 $16$ 通常就足够了。

如果是复杂的逻辑推理或学习全新的行业知识，$r$ 可能需要调大到 $64$ 甚至更高。

边际递减： 研究表明，$r$ 增加到一定程度后，对模型性能的提升会非常微弱，反而会增加过拟合的风险和显存开销。通常 8 或 16 是大多数场景的性价比之选。

缩放因子 $\alpha$ (Alpha) 的选择$\alpha$ 是一个常数，用于对 LoRA 更新进行缩放。

实际的更新量公式为：$$\Delta W = \frac{\alpha}{r} \times (B \times A)$$

设置建议： 在很多实践中，推荐将 $\alpha$ 设置为 $r$ 的两倍（例如 $r=16, \alpha=32$）。

作用： $\alpha$ 就像是微调的“音量键”。当改变 $r$ 时，通过调整 $\alpha$，可以保持更新的幅度相对稳定，而不需要大幅重新调整学习率（Learning Rate）。

# QLoRA

QLoRA 的三大核心支柱：

① 4-bit NormalFloat (NF4) 数据类型

传统的量化（如 Int8）是平均分配刻度的。但神经网络的权重通常呈正态分布（中间多，两头少）。

NF4 专门为这种分布设计。它在权重集中的区域（靠近 0 的地方）刻度更密，在边缘刻度更稀。这使得 4-bit 的压缩几乎能达到 16-bit 的精度。

② 双重量化 (Double Quantization)

量化本身也需要消耗显存（存储缩放系数）。

QLoRA 对这些缩放系数进行了第二次量化（从 32-bit 压到 8-bit）。虽然单次节省不多，但在处理超长上下文和超大参数量时，这能额外抠出几百 MB 的显存。

③ 分页优化器 (Paged Optimizers)

这解决了“显存瞬时爆炸”的问题。

当某次计算需要的显存超过硬件上限时，QLoRA 会利用 NVIDIA 的统一内存技术，将部分显存临时交换到 CPU 内存（RAM）上，等需要时再换回来。这就像给显存加了“虚拟内存”，防止程序直接崩溃（OOM）。

# ppo dpo dapo grpo 
1. PPO (Proximal Policy Optimization)：
1. 
它需要四个模型同时运行（策略模型、参考模型、奖励模型、评论员模型）。

训练逻辑：学生写作文（策略模型）。教练给作文打分（奖励模型 Reward Model）。**评论员（Critic 网络）**预估这篇作文能得多少分。学生根据得分和预估分的差距，小心翼翼地更新写法，确保新旧写法差异不会太大

缺点： 极其耗费显存（要开 4 个模型），计算开销巨大。

2. DPO (Direct Preference Optimization)

既然奖励模型也是学出来的，为什么不直接让模型学习人类的偏好？

原理： 它不需要额外的奖励模型和评论员模型。

训练逻辑：给模型一对数据：一个好答案（$y_w$），一个差答案（$y_l$）。通过数学变换，直接把“好答案比差答案得分高”这个逻辑转换成一个损失函数（Loss Function）。直接拉高好答案的概率，压低差答案的概率。

优点： 简单、稳定，不需要训练奖励模型，显存开销减半。

3. DPAO ：对偏好的精细微调（注：DPAO 通常指在 DPO 基础上引入 Direct Preference Alignment Optimization 或类似的分离式对齐策略）

原理： DPO 有个问题，它有时会过分惩罚那些虽然没被选上但质量也不错的回答。
 
优化逻辑： DPAO 类算法通过调整损失函数的权重，或者引入参考模型的辅助，使得模型在“追求高分”和“保持多样性”之间取得更好的平衡，避免模型变得死板。

4. GRPO (Group Relative Policy Optimization)

它彻底抛弃了 PPO 中最沉重的 Critic（评论员）网络。

原理： 群体相对评分。

训练逻辑：分组采样： 针对一个问题，让模型一口气写 8 篇作文（Group）。

内部打分： 用规则（如：代码能不能跑通、数学结果对不对）或者奖励模型给这 8 篇打分。

相对奖励： 不看绝对分，只看这 8 篇里谁比平均分高。

自我迭代： 比平均分高的就加强，低的就削弱。

优点： * 极省显存： 砍掉了巨大的 Critic 模型，省下的显存可以处理更长的上下文（如 100k）。

自发推理： 这种模式极其适合数学和代码，模型在不断的“组内比对”中自发学会了反思和推导。

# MHA公式

MHA 的核心思想是将模型分为 $h$ 个头，让每个头在不同的子空间学习上下文信息。

单个头的 Attention 计算：对于第 $i$ 个头，其输入为：$$Head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) = \text{softmax}\left(\frac{Q_i K_i^T}{\sqrt{d_k}}\right) V_i$$

多头合并：将所有头的输出拼接（Concat）后，通过一个线性变换 $W^O$ 得到最终结果：$$\text{MHA}(Q, K, V) = \text{Concat}(Head_1, \dots, Head_h)W^O$$

痛点： 随着序列长度增加，KV Cache（缓存 $K$ 和 $V$）会占用海量显存，成为推理时的瓶颈。

更细致的讲解：

对于每一个头 $i$（$i = 1, \dots, h$），我们使用各自独立的权重矩阵 $W$ 将输入 $X$ 映射到不同的子空间：

$$Q_i = X W_i^Q, \quad K_i = X W_i^K, \quad V_i = X W_i^V$$

在每个子空间内，执行标准的点积注意力。记得那个防止梯度消失的 $\sqrt{d_k}$：$$\text{Attention}(Q_i, K_i, V_i) = \text{softmax}\left(\frac{Q_i K_i^T}{\sqrt{d_k}}\right) V_i$$

把所有头计算出来的结果水平拼接在一起。如果每个头的维度是 $d_v$，一共有 $h$ 个头，拼接后的总维度就是 $h \times d_v$：$$\text{MultiHead\_Result} = \text{Concat}(Head_1, Head_2, \dots, Head_h)$$4. 最终线性变换为了让输出的维度重新回到 $d_{model}$，并让各个头的信息进行最后一次融合，我们会乘以一个输出权重矩阵 $W^O$：$$\text{MHA}(Q, K, V) = \text{MultiHead\_Result} \cdot W^O$$

## $W^Q, W^K, W^V$ 是干嘛的？

这里的 $W$ 是“权重矩阵”。

为什么要乘 $W$？ 原始的输入（比如词向量）是固定的，但不同的“头”需要从不同角度看问题。

类比： * 第一个头（Head 1）的 $W$ 矩阵可能让模型关注语法（主语是谁？）。

第二个头（Head 2）的 $W$ 矩阵可能让模型关注情感（它是开心的吗？）。

在模型刚出生（随机初始化）时，$W$ 里的数字全是乱码。它们是通过 反向传播（Backpropagation） 和 梯度下降（Gradient Descent） 这两套组合拳，在数以万亿计的语料中不断修正得到的。

# transformer经典公式

Q, K, V 分别是什么？

$Q$ (Query - 查询)： 就像是你脑子里**“想找的东西”**。例子： 你想找“关于猫的科普书”。

$K$ (Key - 键值)： 就像是书架上每本书的**“标签/书名”**。例子： 书架上有《百科全书》、《宠物养护》、《量子力学》。

$V$ (Value - 数值)： 就像是书里**“具体的内容”**。例子： 《宠物养护》这本书里关于猫的实际知识点。

我们将公式 $$\text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V$$ 

第一步：$Q K^T$（打分/匹配）这是 $Q$ 和 $K$ 的点积。它的目的是计算**“我想要找的（Q）”和“现有的标签（K）”有多像？**如果 $Q$ 和 $K$ 很匹配，得分就高；如果不相关，得分就很低。

结果： 得到一个“相关性分数”矩阵。

第二步：$\div \sqrt{d_k}$（缩放）$d_k$ 是向量的维度。

第三步：$\text{softmax}$（归一化/百分比）把上面的得分转换成 0 到 1 之间的概率，且所有得分加起来等于 100%（即 1）。

结果： 告诉你模型应该把多少“注意力”放在哪本书上。比如：80% 关注《宠物养护》，20% 关注《百科全书》。

第四步：$\times V$（提取信息）最后，用这些百分比去乘以具体的“内容（V）”。

结果： 你得到的是一个加权后的信息。模型通过这个过程，从整个序列中“抓取”了它认为最重要的内容。

# 为什么要除以向量的维度

问题的根源：点积爆炸假设 $Q$ 和 $K$ 的分量都是均值为 0、方差为 1 的独立随机变量。那么它们的点积 $Q \cdot K$ 的均值虽然是 0，但方差会变成 $d_k$（向量维度）。

当 $d_k$ 很大时（比如 512 或 1024），点积的结果可能会变得非常大（正无穷）或非常小（负无穷）。

Softmax 函数的图像像一个拉长了的 S 曲线（在多维空间）。

中间地带： 梯度很大，模型学得很快。

两端（饱和区）： 当输入的值非常大或非常小时，Softmax 的输出会接近 0 或 1，此时它的导数（梯度）几乎为 0。

连锁反应：从“数值大”到“没梯度”如果没有 $\sqrt{d_k}$ 的缩放：$Q K^T$ 计算出了一些非常大的数值。这些大数进入 Softmax 后，结果会变成类似 [0.00001, 0.99998, 0.00001] 这种极端分布。

结果： 在反向传播时，Softmax 处于饱和区，传回去的梯度极其微小。

后果： 权重更新停滞，模型无法学习。这就是典型的梯度消失。

# MQA

第一步：线性变换（投影）这是 MQA 与 MHA 最大的区别。

注意看 $W^K$ 和 $W^V$ 的下标：对于 Query： 每个头仍然有自己的权重矩阵。$$Q_i = X W_i^Q \quad (i = 1, \dots, h)$$

对于 Key 和 Value： 所有头共享唯一的一组权重矩阵。$$K = X W^K, \quad V = X W^V$$

计算注意力 ($Head_i$)每个 $Q_i$ 都去和这同一个 $K$ 匹配，并从这同一个 $V$ 中提取信息：$$Head_i = \text{Attention}(Q_i, K, V) = \text{softmax}\left(\frac{Q_i K^T}{\sqrt{d_k}}\right) V$$

第三步：合并输出$$MQA(X) = \text{Concat}(Head_1, \dots, Head_h) W^O$$

显存开销暴减： 在大模型推理过程中，KV Cache 是最占显存的部分。MQA 将 KV 的存储量减少到了原来的 $1/h$。

推理速度提升： 因为需要读取的 KV 数据量变小了，计算时从内存读取数据（Memory Bandwidth）的瓶颈被打破，推理速度明显变快。

# GQA

GQA (Grouped-Query Attention) 的实现GQA 是 MHA 的优化方案，介于 MHA（多头）和 MQA（Multi-Query Attention，所有 Q 共享一组 KV）之间。

它在保持性能的同时，大幅减少了推理时的显存占用。

实现逻辑：分组： 将 $N$ 个 Query 头分为 $G$ 个组。

共享 KV： 每一组内的所有 Query 头共享 同一组 Key 和 Value 头。

计算：如果 $N=8, G=2$，那么每 4 个 Query 头对应 1 个 Key 头和 1 个 Value 头。在计算时，Key 和 Value 会被“广播”（Broadcast）到组内的每个 Query 上进行点积。

优化点：显存压缩： KV Cache 的大小减少为原来的 $1/G$。

推理加速： 减少了从内存读取 KV 数据的带宽压力（Memory Bandwidth Bound）。

## 如何选择组的大小

目前的行业共识（基于 Llama 3, Mistral 等模型）通常将组大小设定为 4 或 8。

以下是具体的选择逻辑：

 A. 显存瓶颈决定上限如果你的目标是在消费级显卡（如 RTX 4090）上跑大上下文（如 128K context），KV Cache 的压力巨大。

选择较大的组（如 $N:G = 8:1$）： KV Cache 的体积会缩小到原来的 $1/8$。这能让你在有限的显存里塞进更长的对话。

B. 硬件架构决定最优值（Memory Bandwidth）GPU 的计算速度（FLOPS）通常远快于其显存带宽。

在推理时，模型往往被卡在“把数据从显存搬运到计算单元”这一步。

A100/H100 优化： 硬件通常以 32 或 64 字节为单位进行内存对齐。如果 $G$ 选得太小（比如 $G=1$，即 MQA），虽然省了显存，但可能导致内存访问效率（Utilization）下降。

经验法则： 设置 $G$ 使得 KV 头的总维度（$G \times d_k$）能够填满硬件的内存带宽通道。通常 $G$ 设置为 8 是兼顾计算吞吐量和显存的最佳实践。

C.结合模型能力

模型能力决定下限小模型（如 1B-3B）： 本身参数量少，建模能力有限。如果 $G$ 选得太小（组太大），会显著降低模型对复杂长文本的记忆力。建议选较小的组大小（如 2 或 4）。

大模型（如 70B+）： 冗余度高，即使 $G$ 选得比较激进（如 $N:G = 8:1$），性能几乎没有下降。

# 如何防止 LLM 训练过拟合

由于 LLM 参数量巨大，过拟合通常表现为模型在训练集上损失函数极低，在验证集或实际对话中表现僵硬、重复或胡言乱语。

## 训练策略与正则化 (Regularization)

这是在不改变模型基础架构的情况下，最常用的“降温”手段。
 
权重衰减 (Weight Decay): 在损失函数中加入 $L_2$ 正则项，惩罚过大的权重参数。
 
这迫使模型权重保持较小的值，从而使决策边界更平滑。

Dropout: 在训练过程中随机“关闭”一部分神经元。这能防止神经元之间产生过度的共适应（Co-adaptation），迫使模型学习更鲁棒的特征。

早停法 (Early Stopping): 监控验证集的性能。一旦验证集损失不再下降甚至开始上升，立即停止训练。

标签平滑 (Label Smoothing): 防止模型对某一特定输出过于“自信”。通过给正确标签分配略低于 1.0 的概率，给错误标签分配微小的概率，减缓模型过度拟合硬标签的倾向。

## 优化模型参数调整 (Parameter Efficiency)

LoRA (Low-Rank Adaptation): 不直接修改原始模型的所有参数，而是通过学习两个低秩矩阵来捕捉增量更新。这大大减少了可训练参数量，天然具有正则化效果，能显著降低过拟合风险。

冻结层 (Layer Freezing): 在微调时，冻结底层的通用语义表示层，只训练顶层的任务特定层。

## 增强数据的多样性与质量

如果模型“背题”，往往是因为题目太少或重复率太高。

数据去重 (De-duplication): 这是 LLM 预训练中最关键的一步。如果训练集中包含大量重复的网页或代码，模型会倾向于逐字背诵。

数据增强 (Data Augmentation):

回译 (Back-translation): 将文本翻译成另一种语言再翻译回来。

同义词替换: 增加语料的丰富度。

混合训练 (Mixed Task Training): 在特定任务微调时，加入少量的通用预训练数据（Replay/Recall），防止模型在学习新知识时忘掉旧知识（灾难性遗忘），并提升泛化能力。

## 学习率与对齐策略

学习率调度 (LR Scheduling): 使用 Cosine Decay 等策略，在训练后期大幅降低学习率，让模型在局部最优解附近缓慢收敛，避免剧烈震荡导致拟合到噪声上。

RLHF (强化学习人工反馈): 通过人类偏好对齐，纠正模型在纯监督微调（SFT）中可能产生的幻觉或复读机行为。

## 补充：关于$L_2$ 正则项

在原始的损失函数（如交叉熵或均方误差）的基础上，我们添加一个与权重平方和成正比的惩罚项。

假设原始损失函数为 $J_0(\theta)$，引入 $L_2$ 正则化后的总损失函数 $J(\theta)$ 为：$$J(\theta) = J_0(\theta) + \frac{\lambda}{2n} \sum_{w \in \theta} w^2$$

$w$：模型中的权重参数。

$\lambda$ (Lambda)：正则化系数（超参数）。

$\lambda$ 越大，惩罚力度越强。

$n$：样本数量（用于归一化）。

过拟合的特征通常是模型试图通过非常剧烈、复杂的函数波动去穿过每一个训练样本点。在数学上，这意味着权重的绝对值通常会变得非常大。

通过 $L_2$ 惩罚项，模型被迫使 $w$ 趋近于 $0$ 但不等于 $0$。

较小的权重意味着函数的斜率更平缓，对输入特征的噪声不那么敏感，从而提高了泛化能力。

# 梯度消失是什么？

梯度消失 (Vanishing Gradient Problem) 是深度学习训练中的一个经典难题。简单来说，就是在反向传播过程中，随着梯度从输出层向输入层传播，梯度变得越来越小，甚至趋近于 0。

这会导致靠近输入层的网络权重几乎停止更新，模型无法学习到数据中的底层特征，导致训练陷入停滞。

A 激活函数的饱和区早期的神经网络常用 Sigmoid 或 Tanh 函数。Sigmoid 的导数最大值仅为 0.25。当输入较大或较小时，导数极度趋近于 0。在链式相乘的过程中，多个小于 1 的导数连乘，结果会指数级衰减。

B. 权重初始化过小如果初始权重 $|w| < 1$，在层数很深的情况下，权重连乘也会迅速导致梯度归零。

# 如何缓解梯度消失

## 更换激活函数：使用 ReLU 族ReLU (Rectified Linear Unit): 在 $x > 0$ 时，导数恒为 1。

这样在反向传播时，导数连乘不再衰减，保证了梯度能够传回到浅层网络。

进阶版如 GeLU（GPT系列常用）或 SwiGLU（Llama系列常用）也遵循类似的非饱和设计。

## 引入残差连接 (Residual Connections / Skip Connections)这是 Transformer 和 ResNet 成功的关键。

它在层与层之间增加了一条“高速公路”：$y = f(x) + x$。

求导时，$\frac{\partial y}{\partial x} = f'(x) + 1$。

即使 $f'(x)$ 趋近于 0，那个 +1 也能保证梯度无损地流向更深的层。

## 归一化技术 (Normalization)Layer Norm (LN) 或 Batch Norm (BN)

将每一层的输入分布强制拉回到均值为 0、方差为 1 的标准分布。

这使得输入落入激活函数非饱和区（梯度较大的区域）的概率大大增加，避免模型陷入死区。

## 更好的权重初始化Xavier 初始化 或 He 初始化

根据每一层神经元的输入/输出数量来科学设定初始权重的方差，确保信号在传播过程中既不爆炸也不消失

## 门控机制 (Gating Mechanism)在 RNN 领域，通过 LSTM 或 GRU 引入“遗忘门”和“更新门”。

门控结构允许信息有选择地通过，从数学上开辟了一条线性路径来保持长距离的梯度流。

# LoRA 的 A 和 B 怎么初始化，可以换过来吗？

在 LoRA 的原始论文中，设定如下：

矩阵 $A$： 使用 高斯分布（Gaussian Initialization） 或 Kaiming 初始化。

矩阵 $B$： 使用 全零初始化（Zero Initialization）。

这种“一个随机、一个全零”的设计是为了确保：在训练刚刚开始的第一个步（Step 0），LoRA 对原模型的改变量为 0。

如果我们用 $W_0$ 表示预训练权重，$\Delta W$ 表示增量，那么：$$\Delta W = B \times A$$

由于 $B = 0$，那么无论 $A$ 是什么，$B \times A$ 始终等于 $0$。这意味着：$W_{final} = W_0 + 0 = W_0$。

保持基准： 模型从预训练的最优状态开始微调，不会在第一步就因为巨大的随机扰动而“乱阵脚”。避免噪声： 如果 $A$ 和 $B$ 都用高斯初始化，那么 $B \times A$ 会产生一个随机噪声矩阵，叠加在预训练权重上，可能导致模型性能瞬间崩塌，增加收敛难度。

可以换过来吗？（$A=0, B=$高斯）结论：从数学逻辑上是可以的，但在深度学习惯例中不推荐。

虽然 $A=0, B=$高斯 同样能保证初始增量为 $0$，但通常选择 $A$ 作为降维矩阵（Input $\rightarrow$ Rank）进行随机初始化，是因为 $A$ 负责从高维输入中提取特征，而 $B$ 负责将这些特征映射回高维空间。

对称性打破： 在神经网络中，我们通常需要通过随机化来“打破对称性”。

虽然这里 $B$ 是全零，但因为 $A$ 是随机的，梯度更新时 $B$ 很快就会获得非零值，从而开始学习。

# RAG流程

RAG 的五个核心阶段

1、数据准备与索引 (Indexing)这是“离线”完成的预处理过程。

文档加载 (Loading): 读取 PDF、文本、Markdown 等原始文件。

文本切分 (Chunking): 将长文章切成小块（如每块 500 字）。

因为 LLM 的上下文窗口有限，且切分越精细，检索越准。

向量化 (Embedding): 使用 Embedding 模型将文字转为一串数字（向量）。

向量存储 (Vector Store): 将向量和原文存入数据库（如 Pinecone, Milvus, Chroma）。

2. 检索 (Retrieval)当用户提出问题时，系统开始“在线”寻找答案。
 
查询向量化: 将用户的问题（Query）也转化成相同维度的向量。

相似度搜索: 在向量数据库中计算问题向量与文档块向量的距离（通常用余弦相似度）。

召回: 找出最相似的前 $K$ 个文档块（Top-K）。

3. 混合检索与重排序 (Reranking - 进阶步骤)

基础检索有时不准，现代 RAG 通常会增加这一步：

重排序 (Rerank): 使用更精准（但计算慢）的模型对召回的 Top-K 个片段进行二次打分，确保最相关的知识排在最前面。

4. 增强提示词 (Augmentation)将找到的“参考资料”和“用户原问题”塞进一个预设的模板里。

提示词模板示例：你是一个专业的助手。请根据以下参考内容回答问题：

【参考内容】：$Retrieval\_Results$【问题】：$User\_Query$如果参考内容中没有答案，请说不知道，不要胡编乱造。

5. 生成 (Generation)将填充好的提示词发送给 LLM（如 GPT-4 或 Llama 3）。

模型读取了背景知识后，不再依赖“内功”盲猜，而是根据提供的资料生成精准、有据可查的回复。

# 与RAG有关的调参

## chunk size ： 常用 300-500 tokens。如果是法律或代码文档，建议增加到 800+。

## Top-K 的取值 初次检索建议取 $K=10$ 到 $20$。如果直接给 LLM，一般取 $K=3$ 到 $5$，避免无关信息干扰。

## 引入重排序 (Reranker)：向量检索（Vector Search）只管召回相关内容，但不保证“最相关”的排在最前面。

做法： 先粗排（检索出 20 个），再用 Reranker 模型（交叉编码器结构）进行精排，选出前 5 个。

收益： 即使 Embedding 模型较弱，加入一个好的 Reranker（如 bge-reranker）也能显著提升准确率。

# 场景题

## 架构设计类：100 万份文档的私有 RAG 系统

回答核心：分层治理（离线 + 在线）与混合检索。

数据预处理（离线）：

清洗： 必须去除 PDF 解析中的乱码、页眉页脚。

切片： 采用层级切片（Small-to-Big Retrieval），搜到小块（精细），返回给模型大块（上下文全）。

向量化： 选用支持 1024 维以上的国产/开源最优模型（如 BGE-v1.5）。

检索与性能（在线）：

混合检索（Hybrid）： 关键词 (BM25) + 语义 (Vector) 比例 3:7。

重排序（Rerank）： 初筛 50 个，Reranker 精排前 5 个，这是提升准确率最显著的手段。

加速： 使用向量数据库（如 Milvus/Qdrant）的 HNSW 索引，响应控制在 100ms 内。

评估： 引入 RAGAS 指标（忠实度、相关性），建立 50-100 个“黄金问答对”做回归测试。

2. 长文本优化：模型“慢”且“忘”

回答核心：上下文管理与 KV Cache。

解决“慢”：

KV Cache 复用： 相同背景的 Prompt，复用已计算的键值缓存，减少重复计算。

投机采样（Speculative Decoding）： 用小模型先草拟，大模型再校验，速度提升 2-3 倍。

解决“忘”（中间丢失问题）：

滑动窗口（Sliding Window）： 保持最近的注意力。

位置编码优化： 检查模型是否使用了 RoPE 及其外推技术（如 YaRN），确保长距离位置信息不失效。

总结压缩： 将历史对话进行摘要处理，只给模型传“知识点”而非全文。

3. 安全与偏好：不重训纠正偏好

回答核心：护栏（Guardrails）与 Prompt 插值。

输入端限制： 使用敏感词库或“安全分类模型”先过滤用户 Prompt。

System Prompt 强制引导： 在系统提示词中加入“强约束”，例如：“作为法律助手，严禁给出任何主观判断，必须引用法律条文”。

输出端自纠错（Self-Correction）： 让模型生成答案后，内部再进行一次自检：“检查上述回答是否包含歧视倾向，如有请重写”。

解码策略： 降低 Temperature（温度），让输出更稳定、不乱跑。

4. 生产故障：GPU OOM 排查（本地 OK，线上崩）

回答核心：并发环境与动态分配。

多并发压力： 本地通常是单次推理，线上是高并发。排查是否因为 Batching（批处理） 导致显存峰值过高。

上下文长度溢出： 用户输入的长度分布不均。线上可能有极长输入，导致 KV Cache 瞬间占满显存。

显存碎片化： 长期运行后，显存分配不连续。建议使用 vLLM 的 PagedAttention 技术，像操作系统管理内存一样管理显存。

监控工具： 查看 nvidia-smi 的内存占用曲线，确认是加载模型时 OOM（权重太大）还是推理中 OOM（激活值/KV Cache 太大）。

5. 质量评估：垂直领域（医疗/法律）指标构建

回答核心：专业性、合规性与人工闭环。

构建“黄金数据集”： 请资深医生/律师编写 200 个代表性 Case。

特定指标：

医疗： 准确性（Medical Correctness）、安全性（Safety，是否建议及时就医）。

法律： 溯源能力（Citation Accuracy，引用的法条是否真实存在）。

LLM-as-a-Judge： 用 GPT-4o 或更强的模型作为初审裁判，但必须配合 20% 的**人工抽检（Human-in-the-loop）**来对齐人类偏好。

红队测试（Red Teaming）： 故意提问违禁/非法内容，测试模型的边界防御。

# 手撕： 搜索推荐系统 

核心逻辑：漏斗模型 (Retrieval to Ranking)

无论是搜索还是推荐，为了处理海量数据（千万级到十亿级），都遵循**“多级筛选”**的漏斗结构，性能和精度在每一层进行权衡。

召回 (Retrieval)： 从全量库中快速筛选出几千个候选者（速度极快）。

粗排 (Pre-ranking)： 过滤掉明显不符合条件的，剩几百个。

精排 (Ranking)： 使用复杂模型（如 DeepFM, Transformer）进行打分预测（最精准）。

重排 (Re-ranking)： 考虑多样性、去重、广告混排等业务策略。

特性,搜索 (Search),推荐 (Recommendation)

触发点,用户主动输入 Query (意图明确),系统主动预测 User Profile (意图模糊)

核心挑战,语义匹配 (Semantic Matching),兴趣建模 (User Interest)

典型算法,"BM25, TF-IDF, 向量检索","协同过滤, 深度兴趣网络 (DIN)"

实时性,极高，需实时响应搜索词,较高，需捕获用户分钟级的点击行为

# 我现在要做一个客服智能体，更拟人，你会如何做？

## 情绪共鸣引擎 (Emotional Intelligence Layer)

拟人的第一步不是“说对”，而是“感知对”。

多模态情感分析： 不仅分析文字，还要通过语音语调 (Pitch/Tone) 或视频流表情来识别用户的压力等级。

同理心回复策略： 建立专门的 EQ 约束层。

反例： “对不起，查询不到订单。” (机械)

拟人： “非常理解您现在焦急的心情，我刚才仔细查了一下，确实没看到订单信息，咱们对一下单号好吗？” (带温度的确认)

## 动态人格与长期记忆 (Persona & Memory)

人是有性格和记忆的。AI 如果每次对话都像“初次见面”，就永远无法拟人。

人格一致性 (Persona Consistency)： 为智能体设定详细的“职员档案”（如：性格热情的 28 岁资深客服小李）。通过 System Prompt 固定其语言风格（口头禅、语气词的使用频率）。

分层记忆架构：

短期记忆： 缓存当前会话的上下文，避免让用户重复描述问题。

长期记忆 (Long-term Memory)： 将用户的偏好（如：他喜欢简洁的回答）、历史投诉记录存入向量数据库。

效果： “王先生您好，上次您反馈的那个水管漏水问题处理得还好吗？今天有什么能帮您的？”

## 实时推理与“思考”透明化 (System 2 Thinking)

人类在处理复杂问题时会犹豫和思考。2026 年的推理模型（如 DeepSeek R1/OpenAI o-series 架构）提供了这种可能。

主动澄清 (Proactive Clarification)： 拟人的表现之一是“听不懂会问”。

当用户指令模糊时，智能体应主动追问，而不是根据概率瞎猜。

推理链展示 (Thought Trace)： 在等待系统响应时，智能体可以给用户一些中间反馈：“我正在调取您的物流信息，请稍等……奇怪，系统显示包裹在转运中心停留了 2 天，我帮您催一下。”

这种同步思考的过程极大地增强了信任感。

# 怎么缓解 kv cache（几种模型结构）

##  MQA (Multi-Query Attention)原理： 

传统的 Multi-Head Attention (MHA) 为每个 Query Head 都配备独立的 Key 和 Value Head。

MQA 则让所有的 Query Head 共享同一组 Key 和 Value Head。

缓解效果： 将 KV Cache 的显存占用直接降低到原来的 $1/h$（$h$ 为 Head 数量）。

代价： 牺牲了少量的模型表达能力（因为 Key/Value 的多样性降低了）。

## GQA (Grouped-Query Attention)

原理： GQA 是 MHA 和 MQA 的折中方案。它将 Query Head 分组，每一组 Query 共享一组 Key 和 Value Head。

缓解效果： 比如有 32 个 Query Head，每 8 个一组，则只需要 4 组 KV Head。显存占用介于两者之间，但性能几乎能追平 MHA。

优势： 在推理速度和模型质量之间达到了最优平衡。

## MLA (Multi-head Latent Attention)

原理： 这是 DeepSeek-V3/R1 提出的创新结构。它利用低秩压缩 (Low-rank Compression) 技术，将 KV 矩阵投影到一个极小的“潜在向量（Latent Vector）”中。

缓解效果： 通过只存储压缩后的潜在向量（以及少量解压用的矩阵），MLA 将 KV Cache 的存储开销降低到了极低水平，甚至优于 GQA。

优势： 既保证了多头注意力的高容量，又极大地缩减了推理时的显存占用，是目前处理超长上下文的最前沿方案。

## Window Attention & Sliding Window (SWA)

原理： 改变注意力范围。不再让当前 Token 关注前面所有的 Token，而是只关注固定窗口大小（如最近的 4096 个 Token）内的信息。

缓解效果： KV Cache 不再随序列无限增长，而是达到窗口上限后就固定住（类似于一个环形缓冲区）。

# deepspeed 三种stage，分别对哪方面优化，每张卡节省多少（总量/N）

## ZeRO-Stage 1：优化优化器状态 (Optimizer States)这是最常用、性价比最高的阶段。

优化对象： 仅对 Optimizer States（如 Adam 优化器中的 Momentum 和 Variance）进行分片。

显存占用： * 原始状态下，Adam 优化器状态占用的显存非常大（通常是参数量的 12 倍，使用 FP16 训练时）。

在 Stage 1 下，每张卡只负责维护 $1/N$ 的优化器状态。每张卡节省： 总优化器状态显存的 $1/N$。GPU 数量为 $N$

现代大模型训练通常使用更高级的优化器，如 Adam 或 AdamW。

这些优化器不仅看当前的梯度，还要看梯度的历史累积信息，以实现更平滑、更快速的收敛。这些“历史信息”就是 Optimizer States。

在 Adam 优化器中，每更新一个参数 $w$，都需要记录两个额外的状态值：

一阶矩（Momentum/Moving Average of Gradients）： * 记录梯度波动的方向和平均值。类似于物体的“惯性”。

二阶矩（Variance/Moving Average of Squared Gradients）： * 记录梯度平方的平均值。用于根据梯度的稀疏程度自动调节每个参数的学习率。

结论： 对于每一个模型参数 $w$，Adam 优化器都要额外存储 2 个 对应的数值。

模型参数 (Parameters): 使用 FP16 存储，占 2 bytes。

梯度 (Gradients): 使用 FP16 存储，占 2 bytes。

优化器状态 (Optimizer States): * 为了保证精度，优化器内部必须使用 FP32 进行计算和存储。

Adam 的一阶矩 (Momentum)：4 bytes (FP32)。Adam 的二阶矩 (Variance)：4 bytes (FP32)。

此外，通常还会存一份 FP32 的权重副本用于更新：4 bytes (FP32)。总计： 每一个参数需要 $4 + 4 + 4 = 12$ 字节的优化器相关显存。

## ZeRO-Stage 2：优化梯度 (Gradients)

在 Stage 1 的基础上，进一步对梯度进行分片。

优化对象： Optimizer States + Gradients。

原理： 当一个 Bucket 的梯度计算完成后，它们立即被聚合到负责该分片的 GPU 上，其他卡随即释放这部分梯度

。显存占用： * 梯度通常占用 2 倍参数量的空间。现在梯度和优化器状态都变成了原来的 $1/N$。

每张卡节省： (优化器状态 + 梯度) 总量的 $1/N$。

## ZeRO-Stage 3：优化参数 (Parameters)

这是最彻底的优化，支持训练远超单卡显存容量的模型。

优化对象： Optimizer States + Gradients + Parameters。

原理： 连模型参数 $\Psi$ 也不再完整存储在每张卡上。只有在正向或反向传播需要用到某一层时，才通过 Broadcast 通信从其他卡获取。计算完立刻丢弃。

显存占用： * 三大块模型状态全部实现了 $1/N$ 分割。每张卡节省： 模型总状态（参数 + 梯度 + 优化器状态）的 $1/N$。

# 混合精度正常占用多少显存

（12（优化器 3 fp32）+4（参数和梯度 2 bf16）=16）

# 32B 模型调要多少张卡，max_token 有限制吗？(4 node)

# transformer

Transformer 只有两个大箱子：Encoder（编码器） 和 Decoder（解码器）。

Encoder (左边的箱子)： 负责“读懂”。它把输入的句子（比如“我爱编程”）拆解开，搞清楚每个词是什么意思，以及词与词之间是什么关系。

Decoder (右边的箱子)： 负责“写作”。它根据 Encoder 给出的“理解笔记”，一个字一个字地把翻译结果（比如 "I love coding"）写出来。

## Input Embedding 把单词变成向量（数字阵列）:

第一步：词表映射 (Tokenization & ID)我们要先有个“字典”（Vocabulary）。

分词：把句子拆开。比如“我爱编程”拆成 ["我", "爱", "编程"]。

查表：每个词在字典里都有一个唯一的索引（ID）。

“我” → ID: 1“爱” → ID: 2“编程” → ID: 3

第二步：从 ID 到向量 (The Embedding Table)如果直接用 ID（1, 2, 3）给模型看，模型会觉得“编程(3)”比“我(1)”大，这显然没意义。

于是，我们准备一张巨大的数值表格（Embedding Matrix），每一行对应一个词。

维度 (d_model)：论文里默认是 512。也就是说，每一个词都会变成一个包含 512 个数字的数组

初始状态：这些数字刚开始是随机生成的。学习过程：随着模型不断训练，这些数字会发生变化

。第三步：语义空间的“站位”这是 Embedding 最神奇的地方。

训练好的向量是有语义距离的：相似的词，距离近：

在 512 维的空间里，“猫”和“狗”这两个向量的指向会非常接近。

逻辑关联：经典例子是 $Vector(\text{King}) - Vector(\text{Man}) + Vector(\text{Woman}) \approx Vector(\text{Queen})$。

在 Transformer 论文里，最常用的衡量方法有两种：

① 欧式距离 (Euclidean Distance)就是中学学过的两点之间的直线距离。公式： $\sqrt{(x_1-x_2)^2 + (y_1-y_2)^2 + \dots}

$直观理解： 两个点在空间里离得有多远。

② 余弦相似度 (Cosine Similarity) ―― 这个不看距离长短，看两个箭头的夹角。

如果两个箭头的夹角很小（几乎重合），余弦值就接近 1，表示它们非常相似。如果两个箭头垂直，余弦值就是 0，表示没啥关系。如果方向相反，值就是 -1。

Embedding 的三个特性

降维：相比于几万维的 One-hot 编码，512 维更加稠密、节省空间。

语义化：让意思相近的词在数学空间上也靠得近。

可学习：Embedding 不是写死的，它是模型参数的一部分，会越练越聪明。

## Positional Encoding

Embedding 向量出来后，会和 Positional Encoding 向量直接相加（Addition）。

不是拼接（Concat），而是对应位置相加。

所以位置编码的维度必须和 Embedding 维度一模一样（默认都是 512）。

### 如果只用简单的整数标注位置，会有两个致命问题：

数值爆炸： 如果句子很长（比如 1000 个词），第 1000 个词的数值会非常大，模型处理起来会“消化不良”（梯度不稳定）。

泛化能力差： 如果模型只见过长度为 50 的句子，面试时突然来个 100 词的，它就不知道 51 到 100 是什么意思了。

### 正余弦函数：

用不同频率的正弦（Sin）和余弦（Cos）函数来表示位置。

$$PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{model}})$$

$$PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{model}})$$

pos (Position) 代表这个词在句子里的“绝对位置”索引。

$d_{model}$（512 维）：每一个词的 Embedding 是一个长度为 512 的数组。

对应的，Positional Encoding 也是一个长度为 512 的数组。

$i$ 的取值范围： 从 $0$ 到 $255$。公式里用的是 $2i$ 和 $2i+1$。

当 $i=0$ 时，计算的是第 $0$ 位和第 $1$ 位；当 $i=1$ 时，计算的是第 $2$ 位和第 $3$ 位；以此类推，直到 $i=255$，计算完第 $510$ 位和第 $511$ 位。

公式分母上的 $10000^{2i/d_{model}}$。随着 $i$ 的增大（从 0 变到 255），分母会变得非常巨大。分母变大 $\rightarrow$ 频率变低 $\rightarrow$ 波长变长。 

模型只要看一眼当前的编码，就能通过简单的数学运算，“推算”出它前后左右位置的编码长什么样。 这种线性变换关系，让模型学习“相对距离”变得轻而易举。

任意两个位置的编码（比如 $PE_{pos}$ 和 $PE_{pos+k}$）之间存在一个固定的线性转换矩阵。这意味着模型在计算注意力时，通过点积运算，可以自动抵消掉绝对位移，从而‘感受到’两个词之间的相对距离 $k$。

### 残差连接与层归一化（Add & Norm）

在 Attention 之后，图中有一个虚线箭头绕过了 Attention，直接连到了加号上。这就是 Add & Norm。

Add（残差连接）：

把 Attention 处理后的结果和原始输入相加。这保证了模型哪怕深达 100 层，最底层的原始信息也能传上来，不会导致“梯度消失”。

Norm（层归一化）：

把数字重新调整到合理的范围（比如均值为 0，方差为 1），防止某些数字太大导致模型崩溃。

### 前馈神经网络（Feed Forward）

在 Attention 和 Norm 之后，每个词还会单独经过一个 Feed Forward 层。

Attention 负责“横向交流”：词与词之间比对。

Feed Forward 负责“纵向深挖”：对每个词提取出的特征进行非线性变换，进一步加强表现力。

### Masked Multi-Head Attetion

在计算 $Q \times K$ 得到分数矩阵后，我们要手动干预一下：

盖板子： 我们把“未来”位置的分数，全部替换成 负无穷大（$-\infty$）。

变零： 经过 Softmax 计算百分比时，任何负无穷大的数都会变成 0。

### FFN

本质上就是两层全连接层（Linear Layers），中间夹着一个激活函数（ReLU 或 GeLU）。

第一步：扩容 (Up-projection)把 $d_{model}$（512 维）映射到一个更高的维度（通常是 2048 维）。

第二步：激活 (Activation)让数据通过一个“过滤器”（比如 ReLU），把没意义的负数砍掉，增加非线性（让模型能处理更复杂的逻辑）。

第三步：压缩 (Down-projection)再把 2048 维变回 512 维，方便交给下一层。

Attention 负责的是信息捕捉（决定看哪个词），但它的运算主要是线性加权求和。

而 FFN 负责的是特征提取与转换。它利用非线性激活函数，给模型提供了‘消化’和‘深化’信息的能力。

如果没有 FFN，模型就只是在不停地搬运和混合信息，而缺乏真正的深度理解和表达能力。

“维度扩张”：为什么要先从 512 变到 2048？因为在更高维的空间里，词的信息更容易被拆解和重组。

# RLHF阶段，讲一下DPO数据构成

DPO 的核心思想是跳过传统 RLHF 中的奖励模型（Reward Model）建模，直接在偏好数据上优化策略。

其数据构成通常被称为 Triplet（三元组）:

Prompt (查询/提示词 $x$): 用户输入的指令。

Chosen (更优回答 $y_w$): 在人类偏好或模型打分中表现更好的回答。

Rejected (较差回答 $y_l$): 针对同一 Prompt，质量较低、有误或不符合人类价值观的回答。

数据关键点:DPO 必须要有对比，通过损失函数拉大 $y_w$ 和 $y_l$ 在模型中的概率差。

# LoRA 对哪些参数做分解？

Attention 层（最常用）： 主要是 $W_q, W_k, W_v$ 和 $W_o$。

对 Attention 层做低秩分解性价比最高。

